# Install required libraries
!pip install pandas matplotlib seaborn spacy

# Import necessary libraries
import pandas as pd
import spacy
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import drive


# Mount Google Drive
drive.mount('/content/Drive')

# Load spaCy's pretrained NER model
nlp = spacy.load("en_core_web_sm")

# Read the dataset from CSV
df = pd.read_csv('/content/Drive/My Drive/ner.csv')

# Function to extract entities from text using spaCy
def extract_entities(text):
    '''The text is passed through the nlp object, which is a spaCy model loaded earlier (en_core_web_sm).
    The nlp object processes the text and returns a Doc object, which contains linguistic annotations such as tokens, parts of speech, and named entities.'''
    doc = nlp(text)
    '''Creates a list of tuples containing the text and label of each named entity found in the Doc object. 
    The 'doc.ents' property is an iterable of named entity Span objects in the document.
    Each Span object has a '.text attribute', which gives the text of the entity, and a '.label_ 'attribute, which gives the label (e.g., PERSON, ORG, GPE).
    'ent.text': The actual text of the named entity.(e.g., a person's name, a location)
    'ent.label_': The label of the named entity (e.g., PERSON, ORG, GPE).
    Collects all the named entities from the text and stores them in a list, where each item in the list is a tuple containing the entity's text and its type'''
    entities = [(ent.text, ent.label_) for ent in doc.ents]
    # Returns the list of tuples containing the extracted entities from the text
    return entities
# Apply the extract_entities function to each sentence in the dataset and storing it in entities
df['entities'] = df['Sentence'].apply(extract_entities)

# Flatten the list of entities for counting

'''df['entities']: This is the column in the DataFrame where each cell contains a list of tuples, with each tuple representing an entity and its label.

for entity_list in df['entities']: This outer loop iterates over each list of entity tuples in the entities column.

for _, label in entity_list: This inner loop iterates over each tuple within the current list.
The tuple is unpacked into _ and label, where _ captures the entity text (which is not used here), and label captures the entity label.

[label for entity_list in df['entities'] for _, label in entity_list]: This creates a new list of all entity labels by flattening the nested lists.'''
all_entities = [label for entity_list in df['entities'] for _, label in entity_list]
print(all_entities)
# Create a DataFrame for entity counts
'''pd.Series(all_entities): Converts the list of all entity labels into a Pandas Series.

.value_counts(): Counts the occurrences of each unique label in the Series and returns a Series sorted by the counts.

pd.DataFrame(...): Converts the resulting Series into a DataFrame.

.reset_index(): Resets the index of the DataFrame so that the counts are in a column rather than the index.'''
entity_counts = pd.DataFrame(pd.Series(all_entities).value_counts()).reset_index()
print(entity_counts)
'''Renames the columns of the DataFrame from 'index' and 'count' to 'Entity' and 'Count' for better readability.
The DataFrame now contains two columns: one for the entity labels and one for their respective counts.'''
entity_counts.columns = ['Entity', 'Count']
print(entity_counts.columns)
# Plot the entity counts
plt.figure(figsize=(10, 6))
sns.barplot(x='Entity', y='Count', data=entity_counts, palette='viridis')
plt.xticks(rotation=45)
plt.title('Entity Count Distribution')
plt.xlabel('Entity Type')
plt.ylabel('Count')
plt.show()
